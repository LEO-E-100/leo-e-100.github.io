---
layout: post
author: Leo
title: Capstone Project
excerpt: Building a news article classifier
---

At General Assembly my last month of the course will be spent working on my 'Capstone' project, this is a project which will bring together all that I have learned and will hopefully be a successful project and will show my abilities.

The idea that I have decided to follow is hopefully going to encompass many of my favourite data tools and principles. I wanted to have a 'Big Data' angle as well as a Natural Language Processing element. 

I settled on a project which would be in three parts.

1. A web scraper that would grab the date, location, headline and content of news articles
2. A script to run NLP analysis and cluster the articles
3. Create a graph data structure of the information and clusters
4. (Time Permitting) Use these clusters to predict future events

It is clear that the quantity of data that will be produced by this is potentially enormous. I hope to scrape most BBC articles and Reuters articles in order to generate a dataset as large as possible. Data this large will likely not be runnable on my laptop. I will therefore have to combine Spark and AWS (I will have to research this further - perhaps Hadoop will be useful?)

I have already created a basic test dataset of 10 articles which will be useful for running some proof of concept tests prior to Christmas. This will hopefully lead to validation of the principles on which I am working. The current code is laid out below with some basic comments added.

```python
# Basic imports that have grown as the project went on 
# Useful for keeping track of dependcies
import requests
import bs4
import feedparser
import pandas as pd
from bs4 import BeautifulSoup
from scrapy.selector import Selector
from pprint import pprint

# Test URL initially - if we can scrape one webpage, we can follow the same formula repeatedly
URL = 'http://www.bbc.co.uk/news/uk-politics-42277040'

# Basic web scraping flow
r = requests.get(URL)
soup = BeautifulSoup(r.text, 'html.parser')
results = soup.find_all('div', class_='story-body')
test_result = results[0]

# Extracting the Headline
headline = test_result.find('h1', class_='story-body__h1').text

# Extracting the date posted
date_posted = test_result.find('div', class_= 'date date--v2').text

# Extracting the article text
article_text = test_result.find('div', class_= 'story-body__inner').text

# The BBC has tags of related articles at the bottom of the article, this will extract them
tags_division = soup.find_all('div', class_='tags-container')

# Create a list of all tags in the division
tags = []
for i in range(len(tags_division)):
    temp = tags_division[i].find_all('a')
    temp_1 = temp[0].text
    tags.append(temp_1)

# Creating a data dictionary of the extracted data
# allows the extracted data to be formed into a dataframe
data_dict = {
    'headline' : headline,
    'date' : date_posted,
    'tags' : [tags],
    'text' : article_text
}

# Ultimately unused list of the column headings
column_heads = ['headline', 'date', 'tags', 'text']

# The final test dataframe
test_df = pd.DataFrame(data=data_dict)

# BBC has an RSS feed which I can use to grab some articles
# This cell will simply use it for headlines initially
feed = feedparser.parse('http://feeds.bbci.co.uk/news/world/rss.xml')
headlines = []
for i in range(10):
    temp = feed.entries[i].title
    headlines.append(temp)
print headlines

# Using the methods above we can create a simple function which 
# will allow the scraping to become easier down the line
def scrape(URL):
    r = requests.get(URL)
    soup = BeautifulSoup(r.text, 'html.parser')
    results = soup.find_all('div', class_='story-body')
    test_result = results[0]
    headline = test_result.find('h1', class_='story-body__h1').text
    date_posted = test_result.find('div', class_= 'date date--v2').text
    article_text = test_result.find('div', class_= 'story-body__inner').text
    
    tags = scrape_tags(soup)
    
    return headline, date_posted, tags, article_text

# Separate the slightly more complex tags scraping as this deserves its own function
# This function is called within the larger scraping function
def scrape_tags(s):
    tags_division = s.find_all('div', class_='tags-container')
    tags = []
    for i in range(len(tags_division)):
        temp = tags_division[i].find_all('a')
        temp_1 = temp[0].text
        tags.append(temp_1)
    return tags

# Creating the dataframe is the final step - at this point this function doesn't work
def dataframify(dictionary):
    
    column_heads = dictionary.keys()
    
    return pd.DataFrame(data=dictionary, columns=column_heads)

# Using the function requires a useful RSS feeder function to set up the RSS feed
RSS_URL = 'http://feeds.bbci.co.uk/news/world/rss.xml'

def RSS_reader(URL):
    feed = feedparser.parse(URL)
    links = []
    for i in range(len(feed.entries)):
        temp = feed.entries[i].link
        links.append(temp)
    return links

links = RSS_reader(RSS_URL)
new_links = [x.encode('ascii', 'ignore') for x in links]
print len(new_links)

df = pd.DataFrame(columns=column_heads)

for link in range(10):
    print link
    print new_links[link]
    temp_headline, temp_date_posted, temp_tags, temp_article_text = scrape(new_links[link])

    temp_list = [temp_headline, temp_date_posted, temp_tags, temp_article_text]

    df.loc[link] = temp_list

# df.set_value(temp_headline, temp_date_posted, temp_tags, temp_article_text)

df.head()
```