---
layout: post
author: Leo
title: Custom News App
excerpt: Building a Custom news feed based on previous sentiment
---
# Building A Custom News Feed

Principle is to build an email service app that will deliver a daily email with 5 recommended articles based on what articles are in a persons pocket account (labelled dataset). This will initally mean creating a model based on data pulled from pocket using the API. Laterly this will mean creating a service that will pull news from RSS feeds and testing them against the model to find top recommendations. Then creating a second service that will send out automatic emails with recommended articles.

```python
import requests
import pandas as pd
import json
import config
pd.set_option('display.max_colwidth', 200)
```

## Creating supervised dataset

Using the Pocket API to create the initial dataset. Having curated a dataset by adding ~200 articles to pocket and tagged them with 'y' or 'n' depending on whether the user is interested in the article. Ultimately this will give a dataframe of article urls and a label of whether the user is interested or not.

```python
# API authorisation. Once Access Key is known no need to re-run.
auth_params = {
    'consumer_key' : config.consumer_key, 
    'redirect_uri' : 'https://twitter.com/leojpedwards'
}
tkn = requests.post('https://getpocket.com/v3/oauth/request', data=auth_params)
string_tkn = str(tkn.content)
split_tkn = string_tkn.split('=')[1].replace('\'', '')
```

```python
usr_params = {
    'consumer_key' : config.consumer_key, 
    'code' : split_tkn
}
usr = requests.post('https://getpocket.com/v3/oauth/authorize', data=usr_params)
usr.content
```

```python
no_params = {
    'consumer_key' : config.consumer_key,
    'access_token' : config.access_token,
    'tag' : 'n'
}
no_result = requests.post('https://getpocket.com/v3/get', data=no_params)
```

```python
no_json = no_result.json()
no_list = no_json['list']
no_urls = []
for i in no_list.values():
    no_urls.append(i.get('resolved_url'))
```

```python
no_df = pd.DataFrame(no_urls, columns=['urls'])
no_df = no_df.assign(wanted = lambda x: 'n')
no_df.head()
```

```python
yes_params = {
    'consumer_key' : config.consumer_key,
    'access_token' : config.access_token,
    'tag' : 'y'
}
yes_result = requests.post('https://getpocket.com/v3/get', data=yes_params)
```

```python
yes_json = yes_result.json()
yes_list = no_json['list']
yes_urls = []
for i in yes_list.values():
    yes_urls.append(i.get('resolved_url'))
```

```python
yes_df = pd.DataFrame(yes_urls, columns=['urls'])
yes_df = yes_df.assign(wanted = lambda x: 'y')
yes_df.head()
```

```python
df = pd.concat([yes_df, no_df])
df.dropna(inplace=True)
df.head()
```

## Scraping article content

Once the URLs have been recovered it is necessary to scrape the text from these articles in order to carry out the NLP steps. This requires the use of scraping on a number of different web sources. This would be a time consuming process if I were to write a bespoke web scraper for each website. Therefore I decided to use an link embedding service with an api to query. Initially I tried to use embed.ly however this is now a very expensive paid service. I instead have used embed.rocks. This is tested below and applied to the whole list of article URLs from above. 

Once the raw HTML has been extracted from the API, it was necessary to add just the text as a new column which is done using BeautifulSoup.

```python
from bs4 import BeautifulSoup
def get_text(x):
    soup = BeautifulSoup(x, 'lxml')
    text = soup.get_text()
    return text
df.loc[:, 'text'] = df['html'].map(get_text)
df.dropna(inplace=True)
df.head()
```

```python
df.shape
```




    (154, 4)



## Natural Language Processing

From the text column it is possible to call a vectorizer in order to turn the text data into a usable matrix format for Machine Learning. 



```python
from sklearn.feature_extraction.text import TfidfVectorizer
vect = TfidfVectorizer(ngram_range=(1,3), stop_words='english', min_df=3)
vector_matrix = vect.fit_transform(df['text'])
```


```python
vector_matrix
```




    <154x2906 sparse matrix of type '<class 'numpy.float64'>'
    	with 19228 stored elements in Compressed Sparse Row format>



## Support Vector Machines

Building a Support Vector Machine model from the vectorised data. This step will require some evaluation of the quality of the model which has not yet been done as initially it was thought that the iterative process would ensure that the model was effective.


```python
from sklearn.svm import LinearSVC, SVC
clf = SVC()
model = clf.fit(vector_matrix, df['wanted'])
```


```python
X = vector_matrix
y = df['wanted']
```


```python
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
import numpy as np

X_train, X_test, y_train, y_test = train_test_split(X, y)

params = {
    "C" : [1],
    "kernel" : ['rbf'],
    "gamma" : np.linspace(0, 100, 10)
}

gridSearch = GridSearchCV(clf, params, cv=5, n_jobs=1, verbose=1)
model = gridSearch.fit(X_train, y_train)
```

    Fitting 5 folds for each of 10 candidates, totalling 50 fits


    [Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.8s finished



```python
print (model.best_params_)
best_model = model.best_estimator_
best = best_model.fit(X_train, y_train)
score = best.score(X_test, y_test)
print ('Score:\t', score)
```

    {'kernel': 'rbf', 'C': 1, 'gamma': 0.0}
    Score:	 0.358974358974359



```python
params_2 = {
    "C" : np.logspace(-3, 2, 10),
    "kernel" : ['linear', 'poly', 'rbf'],
    "gamma" : np.logspace(-5, 2, 10)
}

gridSearch = GridSearchCV(clf, params_2, cv=5, n_jobs=-1, verbose=1)
model_2 = gridSearch.fit(X_train, y_train)
```

    Fitting 5 folds for each of 300 candidates, totalling 1500 fits


    [Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s
    [Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    4.0s finished



```python
print (model_2.best_params_)
best_model_2 = model_2.best_estimator_
best_2 = best_model_2.fit(X_train, y_train)
score_2 = best_2.score(X_test, y_test)
print ('Score:\t', score_2)
```

    {'kernel': 'linear', 'C': 0.001, 'gamma': 1e-05}
    Score:	 0.358974358974359



```python
clf_2 = LinearSVC()

params = {
    "penalty" : ['l2'],
    "loss" : ['hinge', 'squared_hinge'],
    "C" : np.logspace(-3, 2, 10)
}

gridSearch = GridSearchCV(clf_2, params, cv=5, n_jobs=1, verbose=1)
model_3 = gridSearch.fit(X_train, y_train)
```

    Fitting 5 folds for each of 20 candidates, totalling 100 fits


    [Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished



```python
print (model_3.best_params_)
best_model_3 = model_3.best_estimator_
best_3 = best_model_3.fit(X_train, y_train)
score_3 = best_3.score(X_test, y_test)
print ('Score:\t', score_3)
```

    {'C': 0.001, 'penalty': 'l2', 'loss': 'hinge'}
    Score:	 0.358974358974359


### Test Gspread credentials


```python
import gspread
from oauth2client.service_account import ServiceAccountCredentials

json_file = 'Custom News Feed-d7876b12a476.json'

scope = ['https://spreadsheets.google.com/feeds']

credentials = ServiceAccountCredentials.from_json_keyfile_name(json_file, scope)

connection = gspread.authorize(credentials)
```


```python
NF_file = connection.open('NewsFeed')
NF_sheet = NF_file.sheet1
NF_list = list(zip(NF_sheet.col_values(2), NF_sheet.col_values(3), NF_sheet.col_values(4)))
articles_df = pd.DataFrame(NF_list, columns=['title', 'urls', 'html'])
articles_df.replace('', pd.np.nan, inplace=True)
articles_df.dropna(inplace=True)
articles_df.head()
```

```python
articles_df.loc[:, 'text'] = articles_df['html'].map(get_text) 
articles_df.reset_index(drop=True, inplace=True)
test_matrix = vect.transform(articles_df['text'])
test_matrix
```




    <5x2873 sparse matrix of type '<class 'numpy.float64'>'
    	with 42 stored elements in Compressed Sparse Row format>




```python
results = pd.DataFrame(model.predict(test_matrix), columns = ['wanted'])
```


```python
results
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>wanted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>n</td>
    </tr>
    <tr>
      <th>1</th>
      <td>y</td>
    </tr>
    <tr>
      <th>2</th>
      <td>y</td>
    </tr>
    <tr>
      <th>3</th>
      <td>y</td>
    </tr>
    <tr>
      <th>4</th>
      <td>y</td>
    </tr>
  </tbody>
</table>
</div>




```python
rez = pd.merge(results, articles_df, left_index=True, right_index=True)
rez
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>wanted</th>
      <th>title</th>
      <th>urls</th>
      <th>html</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>n</td>
      <td>The Pogues frontman's star-studded 60th birthday</td>
      <td>http://www.bbc.co.uk/news/world-europe-42701430</td>
      <td>The man who brought punk and Irish folk together, turned 60 with a star-studded concert.</td>
      <td>The man who brought punk and Irish folk together, turned 60 with a star-studded concert.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>y</td>
      <td>John Worboys: Rapist's jail move bid refused in 2015</td>
      <td>http://www.bbc.co.uk/news/uk-42700475</td>
      <td>Parole Board confirms it denied open jail transfer two years before ruling it was safe to free rapist.</td>
      <td>Parole Board confirms it denied open jail transfer two years before ruling it was safe to free rapist.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>y</td>
      <td>Syrian opposition calls on Trump and EU to put pressure on Russia and Iran</td>
      <td>http://www.reuters.com/article/us-mideast-crisis-syria-opposition/syrian-opposition-calls-on-trump-and-eu-to-put-pressure-on-russia-and-iran-idUSKBN1F51LO?feedType=RSS&amp;feedName=worldNews</td>
      <td>LONDON (Reuters) - U.S. President Donald Trump and European Union leaders should increase the pressure on President Bashar al-Assad and his allies Russia and Iran to return to talks to end Syria's...</td>
      <td>LONDON (Reuters) - U.S. President Donald Trump and European Union leaders should increase the pressure on President Bashar al-Assad and his allies Russia and Iran to return to talks to end Syria's...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>y</td>
      <td>Does MP Sir Desmond Swayne nod off in Ken Clarke's speech?</td>
      <td>http://www.bbc.co.uk/news/uk-politics-42708212</td>
      <td>Does MP Sir Desmond Swayne nod off during Ken Clarke's speech on the EU Withdrawal Bill?</td>
      <td>Does MP Sir Desmond Swayne nod off during Ken Clarke's speech on the EU Withdrawal Bill?</td>
    </tr>
    <tr>
      <th>4</th>
      <td>y</td>
      <td>England's first 'prisoner of war' discovered</td>
      <td>http://www.bbc.co.uk/news/education-42690437</td>
      <td>A French aristocrat, captured in 1357, was England's earliest official "prisoner of war", say historians.</td>
      <td>A French aristocrat, captured in 1357, was England's earliest official "prisoner of war", say historians.</td>
    </tr>
  </tbody>
</table>
</div>



### Tune the Model


```python
# Hypothetical correction method
change_to_no = [1, 3, 4]

change_to_yes = []
```


```python
for i in rez.iloc[change_to_yes].index:
    rez.iloc[i]['wanted'] = 'y'
for i in rez.iloc[change_to_no].index:
    rez.iloc[i]['wanted'] = 'n'
rez
```

```python
combined = pd.concat([df[['wanted', 'text']], rez[['wanted', 'text']]])
combined
```

```python
# Rebuild model with new data
tvcomb = vect.fit_transform(combined['text'], combined['wanted'])
model = clf.fit(tvcomb, combined['wanted'])
# Iterate this process
```

### Output the Model to Pickle


```python
import pickle
pickle.dump(model, open(r'news_model_pickle.pkl', 'wb'))
pickle.dump(vect, open(r'news_vect_pickle.pkl', 'wb'))
```

## TODO

- ~~Add news article to supervised dataset~~
- ~~More articles in supervised dataset~~
- Flask web app
- ~~Add config variables to config file~~
- Score top articles rather than simple yes/no
- Setup script for email service